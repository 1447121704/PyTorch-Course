{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            \n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"nmt/en-cn/train.txt\"\n",
    "dev_file = \"nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 1\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = 0\n",
    "    word_dict[\"PAD\"] = 1\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把单词全部转变成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把全部句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "#     x_mask = np.zeros((n_samples, max_len)).astype('float32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "#         x_mask[idx, :lengths[idx]] = 1.0\n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 475, 4, 3]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS', 'run', '.', 'EOS']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_en_dict[idx] for idx in train_en[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据全部处理完成，现在我们开始构建seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        # code.interact(local=locals())\n",
    "        \n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        # self.bilinear_attn = nn.Bilinear(enc_hidden_size, dec_hidden_size, 1, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, enc_hidden_size\n",
    "    \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                batch_size, input_len, -1) # batch_size, output_len, dec_hidden_size\n",
    "        # code.interact(local=locals())\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) # batch_size, output_len, context_len\n",
    "\n",
    "        \n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2) # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context) # batch_size, output_len, enc_hidden_size\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        \n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "        \n",
    "        \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        # code.interact(local=locals())\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "#             print(output.shape, output.max(1)[1])\n",
    "            preds.append(output.max(2)[1].view(batch_size, 1))\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "en_vocab_size = len(en_dict)\n",
    "cn_vocab_size = len(cn_dict)\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_vocab_size, \n",
    "                  embed_size=embed_size, \n",
    "                  enc_hidden_size=hidden_size,\n",
    "                  dec_hidden_size=hidden_size)\n",
    "decoder = Decoder(vocab_size=cn_vocab_size, \n",
    "                  embed_size=embed_size, \n",
    "                  enc_hidden_size=hidden_size,\n",
    "                  dec_hidden_size=hidden_size)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "crit = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 loss 8.081835746765137\n",
      "epoch 0 iteration 100 loss 5.214842796325684\n",
      "epoch 0 iteration 200 loss 5.302420139312744\n",
      "epoch 1 iteration 0 loss 6.014657974243164\n",
      "epoch 1 iteration 100 loss 4.765828609466553\n",
      "epoch 1 iteration 200 loss 4.884742259979248\n",
      "epoch 2 iteration 0 loss 5.0775980949401855\n",
      "epoch 2 iteration 100 loss 4.300010681152344\n",
      "epoch 2 iteration 200 loss 4.5046844482421875\n",
      "epoch 3 iteration 0 loss 4.6680498123168945\n",
      "epoch 3 iteration 100 loss 3.9544708728790283\n",
      "epoch 3 iteration 200 loss 4.194519519805908\n",
      "epoch 4 iteration 0 loss 4.399341106414795\n",
      "epoch 4 iteration 100 loss 3.659848213195801\n",
      "epoch 4 iteration 200 loss 3.97265625\n",
      "epoch 5 iteration 0 loss 4.066376686096191\n",
      "epoch 5 iteration 100 loss 3.4122064113616943\n",
      "epoch 5 iteration 200 loss 3.757399082183838\n",
      "epoch 6 iteration 0 loss 3.754815101623535\n",
      "epoch 6 iteration 100 loss 3.205397605895996\n",
      "epoch 6 iteration 200 loss 3.5767199993133545\n",
      "epoch 7 iteration 0 loss 3.540644645690918\n",
      "epoch 7 iteration 100 loss 3.0053226947784424\n",
      "epoch 7 iteration 200 loss 3.4395909309387207\n",
      "epoch 8 iteration 0 loss 3.3088600635528564\n",
      "epoch 8 iteration 100 loss 2.8546128273010254\n",
      "epoch 8 iteration 200 loss 3.3438916206359863\n",
      "epoch 9 iteration 0 loss 3.2211110591888428\n",
      "epoch 9 iteration 100 loss 2.7148079872131348\n",
      "epoch 9 iteration 200 loss 3.1534318923950195\n",
      "epoch 10 iteration 0 loss 2.9978647232055664\n",
      "epoch 10 iteration 100 loss 2.541321039199829\n",
      "epoch 10 iteration 200 loss 3.100534677505493\n",
      "epoch 11 iteration 0 loss 2.7921485900878906\n",
      "epoch 11 iteration 100 loss 2.462599754333496\n",
      "epoch 11 iteration 200 loss 3.0103955268859863\n",
      "epoch 12 iteration 0 loss 2.6914613246917725\n",
      "epoch 12 iteration 100 loss 2.3549747467041016\n",
      "epoch 12 iteration 200 loss 2.9553062915802\n",
      "epoch 13 iteration 0 loss 2.5650887489318848\n",
      "epoch 13 iteration 100 loss 2.263852119445801\n",
      "epoch 13 iteration 200 loss 2.875244140625\n",
      "epoch 14 iteration 0 loss 2.4056499004364014\n",
      "epoch 14 iteration 100 loss 2.1760213375091553\n",
      "epoch 14 iteration 200 loss 2.768026828765869\n",
      "epoch 15 iteration 0 loss 2.3271849155426025\n",
      "epoch 15 iteration 100 loss 2.0930795669555664\n",
      "epoch 15 iteration 200 loss 2.7734038829803467\n",
      "epoch 16 iteration 0 loss 2.1753933429718018\n",
      "epoch 16 iteration 100 loss 2.019136905670166\n",
      "epoch 16 iteration 200 loss 2.72402024269104\n",
      "epoch 17 iteration 0 loss 2.125516891479492\n",
      "epoch 17 iteration 100 loss 1.9290555715560913\n",
      "epoch 17 iteration 200 loss 2.595089912414551\n",
      "epoch 18 iteration 0 loss 2.026808977127075\n",
      "epoch 18 iteration 100 loss 1.8828619718551636\n",
      "epoch 18 iteration 200 loss 2.562403440475464\n",
      "epoch 19 iteration 0 loss 1.9332529306411743\n",
      "epoch 19 iteration 100 loss 1.8373889923095703\n",
      "epoch 19 iteration 200 loss 2.5712478160858154\n",
      "epoch 20 iteration 0 loss 1.8568317890167236\n",
      "epoch 20 iteration 100 loss 1.791002869606018\n",
      "epoch 20 iteration 200 loss 2.441854476928711\n",
      "epoch 21 iteration 0 loss 1.781612753868103\n",
      "epoch 21 iteration 100 loss 1.739814043045044\n",
      "epoch 21 iteration 200 loss 2.459763526916504\n",
      "epoch 22 iteration 0 loss 1.653356671333313\n",
      "epoch 22 iteration 100 loss 1.7056405544281006\n",
      "epoch 22 iteration 200 loss 2.3410844802856445\n",
      "epoch 23 iteration 0 loss 1.660791039466858\n",
      "epoch 23 iteration 100 loss 1.6126714944839478\n",
      "epoch 23 iteration 200 loss 2.345243453979492\n",
      "epoch 24 iteration 0 loss 1.5288070440292358\n",
      "epoch 24 iteration 100 loss 1.6006799936294556\n",
      "epoch 24 iteration 200 loss 2.2646491527557373\n",
      "epoch 25 iteration 0 loss 1.4788926839828491\n",
      "epoch 25 iteration 100 loss 1.56551194190979\n",
      "epoch 25 iteration 200 loss 2.290555477142334\n",
      "epoch 26 iteration 0 loss 1.4501782655715942\n",
      "epoch 26 iteration 100 loss 1.5155285596847534\n",
      "epoch 26 iteration 200 loss 2.200547695159912\n",
      "epoch 27 iteration 0 loss 1.3645713329315186\n",
      "epoch 27 iteration 100 loss 1.4658359289169312\n",
      "epoch 27 iteration 200 loss 2.222991943359375\n",
      "epoch 28 iteration 0 loss 1.3109972476959229\n",
      "epoch 28 iteration 100 loss 1.440930724143982\n",
      "epoch 28 iteration 200 loss 2.1947672367095947\n",
      "epoch 29 iteration 0 loss 1.2702305316925049\n",
      "epoch 29 iteration 100 loss 1.4272795915603638\n",
      "epoch 29 iteration 200 loss 2.149167537689209\n",
      "epoch 30 iteration 0 loss 1.227664589881897\n",
      "epoch 30 iteration 100 loss 1.324533462524414\n",
      "epoch 30 iteration 200 loss 2.109508991241455\n",
      "epoch 31 iteration 0 loss 1.2349250316619873\n",
      "epoch 31 iteration 100 loss 1.377989649772644\n",
      "epoch 31 iteration 200 loss 2.081718683242798\n",
      "epoch 32 iteration 0 loss 1.173404574394226\n",
      "epoch 32 iteration 100 loss 1.2946282625198364\n",
      "epoch 32 iteration 200 loss 2.0912749767303467\n",
      "epoch 33 iteration 0 loss 1.0952588319778442\n",
      "epoch 33 iteration 100 loss 1.3393205404281616\n",
      "epoch 33 iteration 200 loss 2.0052192211151123\n",
      "epoch 34 iteration 0 loss 1.029144525527954\n",
      "epoch 34 iteration 100 loss 1.2892440557479858\n",
      "epoch 34 iteration 200 loss 1.9980454444885254\n",
      "epoch 35 iteration 0 loss 1.0226792097091675\n",
      "epoch 35 iteration 100 loss 1.2825158834457397\n",
      "epoch 35 iteration 200 loss 1.9890691041946411\n",
      "epoch 36 iteration 0 loss 1.029274582862854\n",
      "epoch 36 iteration 100 loss 1.2062729597091675\n",
      "epoch 36 iteration 200 loss 1.920265793800354\n",
      "epoch 37 iteration 0 loss 1.027186632156372\n",
      "epoch 37 iteration 100 loss 1.272233247756958\n",
      "epoch 37 iteration 200 loss 1.9453433752059937\n",
      "epoch 38 iteration 0 loss 0.961983323097229\n",
      "epoch 38 iteration 100 loss 1.1682538986206055\n",
      "epoch 38 iteration 200 loss 1.8981170654296875\n",
      "epoch 39 iteration 0 loss 0.9819805026054382\n",
      "epoch 39 iteration 100 loss 1.1627812385559082\n",
      "epoch 39 iteration 200 loss 1.8635613918304443\n",
      "epoch 40 iteration 0 loss 0.9074685573577881\n",
      "epoch 40 iteration 100 loss 1.1580010652542114\n",
      "epoch 40 iteration 200 loss 1.835594654083252\n",
      "epoch 41 iteration 0 loss 0.8373957872390747\n",
      "epoch 41 iteration 100 loss 1.0702815055847168\n",
      "epoch 41 iteration 200 loss 1.8272783756256104\n",
      "epoch 42 iteration 0 loss 0.9566553235054016\n",
      "epoch 42 iteration 100 loss 1.094472050666809\n",
      "epoch 42 iteration 200 loss 1.828141450881958\n",
      "epoch 43 iteration 0 loss 0.8739888668060303\n",
      "epoch 43 iteration 100 loss 1.0399715900421143\n",
      "epoch 43 iteration 200 loss 1.8393981456756592\n",
      "epoch 44 iteration 0 loss 0.8398782014846802\n",
      "epoch 44 iteration 100 loss 1.0982861518859863\n",
      "epoch 44 iteration 200 loss 1.755192518234253\n",
      "epoch 45 iteration 0 loss 0.7885847687721252\n",
      "epoch 45 iteration 100 loss 1.0420420169830322\n",
      "epoch 45 iteration 200 loss 1.7909458875656128\n",
      "epoch 46 iteration 0 loss 0.8077581524848938\n",
      "epoch 46 iteration 100 loss 1.0192139148712158\n",
      "epoch 46 iteration 200 loss 1.8097929954528809\n",
      "epoch 47 iteration 0 loss 0.7451819777488708\n",
      "epoch 47 iteration 100 loss 1.0203611850738525\n",
      "epoch 47 iteration 200 loss 1.8100937604904175\n",
      "epoch 48 iteration 0 loss 0.7770861983299255\n",
      "epoch 48 iteration 100 loss 0.9532226324081421\n",
      "epoch 48 iteration 200 loss 1.6958603858947754\n",
      "epoch 49 iteration 0 loss 0.7740640640258789\n",
      "epoch 49 iteration 100 loss 1.0237451791763306\n",
      "epoch 49 iteration 200 loss 1.6622179746627808\n",
      "epoch 50 iteration 0 loss 0.7500290870666504\n",
      "epoch 50 iteration 100 loss 0.9715209603309631\n",
      "epoch 50 iteration 200 loss 1.6937726736068726\n",
      "epoch 51 iteration 0 loss 0.7147203087806702\n",
      "epoch 51 iteration 100 loss 0.9316775798797607\n",
      "epoch 51 iteration 200 loss 1.6785792112350464\n",
      "epoch 52 iteration 0 loss 0.736721396446228\n",
      "epoch 52 iteration 100 loss 0.9160623550415039\n",
      "epoch 52 iteration 200 loss 1.6326853036880493\n",
      "epoch 53 iteration 0 loss 0.7036211490631104\n",
      "epoch 53 iteration 100 loss 0.8946568965911865\n",
      "epoch 53 iteration 200 loss 1.6245287656784058\n",
      "epoch 54 iteration 0 loss 0.6954941153526306\n",
      "epoch 54 iteration 100 loss 0.9163867235183716\n",
      "epoch 54 iteration 200 loss 1.5607420206069946\n",
      "epoch 55 iteration 0 loss 0.6372390985488892\n",
      "epoch 55 iteration 100 loss 0.9122201800346375\n",
      "epoch 55 iteration 200 loss 1.6454943418502808\n",
      "epoch 56 iteration 0 loss 0.6235592365264893\n",
      "epoch 56 iteration 100 loss 0.8953887224197388\n",
      "epoch 56 iteration 200 loss 1.5659725666046143\n",
      "epoch 57 iteration 0 loss 0.6208727359771729\n",
      "epoch 57 iteration 100 loss 0.8052783012390137\n",
      "epoch 57 iteration 200 loss 1.5435009002685547\n",
      "epoch 58 iteration 0 loss 0.629677414894104\n",
      "epoch 58 iteration 100 loss 0.8729320168495178\n",
      "epoch 58 iteration 200 loss 1.53748619556427\n",
      "epoch 59 iteration 0 loss 0.5625100135803223\n",
      "epoch 59 iteration 100 loss 0.8544608950614929\n",
      "epoch 59 iteration 200 loss 1.522261381149292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 iteration 0 loss 0.5616536736488342\n",
      "epoch 60 iteration 100 loss 0.8573853969573975\n",
      "epoch 60 iteration 200 loss 1.6121914386749268\n",
      "epoch 61 iteration 0 loss 0.5453298687934875\n",
      "epoch 61 iteration 100 loss 0.786577582359314\n",
      "epoch 61 iteration 200 loss 1.5459955930709839\n",
      "epoch 62 iteration 0 loss 0.534515380859375\n",
      "epoch 62 iteration 100 loss 0.8207312822341919\n",
      "epoch 62 iteration 200 loss 1.4674885272979736\n",
      "epoch 63 iteration 0 loss 0.526824951171875\n",
      "epoch 63 iteration 100 loss 0.8078739047050476\n",
      "epoch 63 iteration 200 loss 1.451117992401123\n",
      "epoch 64 iteration 0 loss 0.5263736844062805\n",
      "epoch 64 iteration 100 loss 0.8131915330886841\n",
      "epoch 64 iteration 200 loss 1.4365651607513428\n",
      "epoch 65 iteration 0 loss 0.5667953491210938\n",
      "epoch 65 iteration 100 loss 0.7984978556632996\n",
      "epoch 65 iteration 200 loss 1.4843101501464844\n",
      "epoch 66 iteration 0 loss 0.5558284521102905\n",
      "epoch 66 iteration 100 loss 0.7762783169746399\n",
      "epoch 66 iteration 200 loss 1.429058313369751\n",
      "epoch 67 iteration 0 loss 0.5241511464118958\n",
      "epoch 67 iteration 100 loss 0.7562233209609985\n",
      "epoch 67 iteration 200 loss 1.4337633848190308\n",
      "epoch 68 iteration 0 loss 0.5350145697593689\n",
      "epoch 68 iteration 100 loss 0.7751164436340332\n",
      "epoch 68 iteration 200 loss 1.3946906328201294\n",
      "epoch 69 iteration 0 loss 0.5043689608573914\n",
      "epoch 69 iteration 100 loss 0.7491386532783508\n",
      "epoch 69 iteration 200 loss 1.3829540014266968\n",
      "epoch 70 iteration 0 loss 0.47860977053642273\n",
      "epoch 70 iteration 100 loss 0.7443416118621826\n",
      "epoch 70 iteration 200 loss 1.3513379096984863\n",
      "epoch 71 iteration 0 loss 0.5359615087509155\n",
      "epoch 71 iteration 100 loss 0.7292683720588684\n",
      "epoch 71 iteration 200 loss 1.371208667755127\n",
      "epoch 72 iteration 0 loss 0.4905513525009155\n",
      "epoch 72 iteration 100 loss 0.7850453853607178\n",
      "epoch 72 iteration 200 loss 1.3778998851776123\n",
      "epoch 73 iteration 0 loss 0.46738511323928833\n",
      "epoch 73 iteration 100 loss 0.7477974891662598\n",
      "epoch 73 iteration 200 loss 1.3959765434265137\n",
      "epoch 74 iteration 0 loss 0.4859740138053894\n",
      "epoch 74 iteration 100 loss 0.7414045929908752\n",
      "epoch 74 iteration 200 loss 1.3407303094863892\n",
      "epoch 75 iteration 0 loss 0.4282393157482147\n",
      "epoch 75 iteration 100 loss 0.7269782423973083\n",
      "epoch 75 iteration 200 loss 1.3731693029403687\n",
      "epoch 76 iteration 0 loss 0.438242107629776\n",
      "epoch 76 iteration 100 loss 0.7301536798477173\n",
      "epoch 76 iteration 200 loss 1.3352558612823486\n",
      "epoch 77 iteration 0 loss 0.45700880885124207\n",
      "epoch 77 iteration 100 loss 0.6689504981040955\n",
      "epoch 77 iteration 200 loss 1.3152105808258057\n",
      "epoch 78 iteration 0 loss 0.4491751492023468\n",
      "epoch 78 iteration 100 loss 0.670512855052948\n",
      "epoch 78 iteration 200 loss 1.3248780965805054\n",
      "epoch 79 iteration 0 loss 0.41375914216041565\n",
      "epoch 79 iteration 100 loss 0.7187517285346985\n",
      "epoch 79 iteration 200 loss 1.217159628868103\n",
      "epoch 80 iteration 0 loss 0.47694119811058044\n",
      "epoch 80 iteration 100 loss 0.6569252610206604\n",
      "epoch 80 iteration 200 loss 1.289062261581421\n",
      "epoch 81 iteration 0 loss 0.4773954749107361\n",
      "epoch 81 iteration 100 loss 0.6322450041770935\n",
      "epoch 81 iteration 200 loss 1.3536189794540405\n",
      "epoch 82 iteration 0 loss 0.40656742453575134\n",
      "epoch 82 iteration 100 loss 0.67964106798172\n",
      "epoch 82 iteration 200 loss 1.2682116031646729\n",
      "epoch 83 iteration 0 loss 0.4054274559020996\n",
      "epoch 83 iteration 100 loss 0.6655434370040894\n",
      "epoch 83 iteration 200 loss 1.203610897064209\n",
      "epoch 84 iteration 0 loss 0.42560046911239624\n",
      "epoch 84 iteration 100 loss 0.7169292569160461\n",
      "epoch 84 iteration 200 loss 1.1884874105453491\n",
      "epoch 85 iteration 0 loss 0.40732118487358093\n",
      "epoch 85 iteration 100 loss 0.7062488794326782\n",
      "epoch 85 iteration 200 loss 1.2066322565078735\n",
      "epoch 86 iteration 0 loss 0.4086015522480011\n",
      "epoch 86 iteration 100 loss 0.6509503126144409\n",
      "epoch 86 iteration 200 loss 1.220201849937439\n",
      "epoch 87 iteration 0 loss 0.4263665974140167\n",
      "epoch 87 iteration 100 loss 0.6854779124259949\n",
      "epoch 87 iteration 200 loss 1.199743390083313\n",
      "epoch 88 iteration 0 loss 0.3770295977592468\n",
      "epoch 88 iteration 100 loss 0.5923302173614502\n",
      "epoch 88 iteration 200 loss 1.2035857439041138\n",
      "epoch 89 iteration 0 loss 0.39905789494514465\n",
      "epoch 89 iteration 100 loss 0.6481903791427612\n",
      "epoch 89 iteration 200 loss 1.165757179260254\n",
      "epoch 90 iteration 0 loss 0.3650065064430237\n",
      "epoch 90 iteration 100 loss 0.6587710380554199\n",
      "epoch 90 iteration 200 loss 1.1977473497390747\n",
      "epoch 91 iteration 0 loss 0.3702786862850189\n",
      "epoch 91 iteration 100 loss 0.6292135119438171\n",
      "epoch 91 iteration 200 loss 1.1143510341644287\n",
      "epoch 92 iteration 0 loss 0.43218567967414856\n",
      "epoch 92 iteration 100 loss 0.6332982182502747\n",
      "epoch 92 iteration 200 loss 1.115442156791687\n",
      "epoch 93 iteration 0 loss 0.4095306396484375\n",
      "epoch 93 iteration 100 loss 0.6071757674217224\n",
      "epoch 93 iteration 200 loss 1.1668881177902222\n",
      "epoch 94 iteration 0 loss 0.39281725883483887\n",
      "epoch 94 iteration 100 loss 0.6433982849121094\n",
      "epoch 94 iteration 200 loss 1.2054331302642822\n",
      "epoch 95 iteration 0 loss 0.3587692379951477\n",
      "epoch 95 iteration 100 loss 0.655164361000061\n",
      "epoch 95 iteration 200 loss 1.1278585195541382\n",
      "epoch 96 iteration 0 loss 0.35450154542922974\n",
      "epoch 96 iteration 100 loss 0.5694862008094788\n",
      "epoch 96 iteration 200 loss 1.1237759590148926\n",
      "epoch 97 iteration 0 loss 0.34589990973472595\n",
      "epoch 97 iteration 100 loss 0.6413514018058777\n",
      "epoch 97 iteration 200 loss 1.135343074798584\n",
      "epoch 98 iteration 0 loss 0.4332904517650604\n",
      "epoch 98 iteration 100 loss 0.7004508376121521\n",
      "epoch 98 iteration 200 loss 1.1826157569885254\n",
      "epoch 99 iteration 0 loss 0.36144059896469116\n",
      "epoch 99 iteration 100 loss 0.5665675401687622\n",
      "epoch 99 iteration 200 loss 1.1054925918579102\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_num_words = total_loss = 0.\n",
    "for epoch in range(num_epochs):\n",
    "    for it, (mb_x, mb_x_lengths, mb_y, mb_y_lengths) in enumerate(train_data):\n",
    "        mb_x = torch.from_numpy(mb_x).long().to(device)\n",
    "        mb_x_lengths = torch.from_numpy(mb_x_lengths).long().to(device)\n",
    "        mb_input = torch.from_numpy(mb_y[:,:-1]).long().to(device)\n",
    "        mb_out = torch.from_numpy(mb_y[:, 1:]).long().to(device)\n",
    "        mb_y_lengths = torch.from_numpy(mb_y_lengths-1).long().to(device)\n",
    "        mb_y_lengths[mb_y_lengths <= 0] = 1\n",
    "        \n",
    "        mb_pred, attn = model(mb_x, mb_x_lengths, mb_input, mb_y_lengths)\n",
    "        \n",
    "        mb_out_mask = torch.arange(mb_y_lengths.max().item(), device=device)[None, :] < mb_y_lengths[:, None]\n",
    "        mb_out_mask = mb_out_mask.float()\n",
    "        # code.interact(local=locals())\n",
    "        loss = crit(mb_pred, mb_out, mb_out_mask)\n",
    "        \n",
    "        num_words = torch.sum(mb_y_lengths).item()\n",
    "        total_loss += loss.item() * num_words\n",
    "        total_num_words += num_words\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            print(\"epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS tom 's daughter pretended not to know him when he came to pick her up from school in his battered old car . EOS UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n"
     ]
    }
   ],
   "source": [
    "idx = mb_x[[1]][:mb_x_lengths[[1]].item()].data.cpu().numpy().reshape(-1)\n",
    "words = [inv_en_dict[i] for i in idx]\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'happended']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS look there . EOS\n",
      "BOS 看 那 里 。 EOS\n",
      "上 在 那 看 看 看 看 看 看 看 看 那 那 那 。 那 那 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[word] for word in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    print(\" \".join([inv_cn_dict[word] for word in dev_cn[i]]))\n",
    "\n",
    "    sent = nltk.word_tokenize(en_sent.lower())\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    mb_x = torch.Tensor([[en_dict.get(w, 0) for w in sent]]).long().to(device)\n",
    "    mb_x_len = torch.Tensor([len(sent)]).long().to(device)\n",
    "    translation, attention = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\" \".join(translation))\n",
    "translate_dev(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
