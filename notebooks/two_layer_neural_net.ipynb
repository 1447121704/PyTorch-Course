{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "什么是PyTorch?\n",
    "================\n",
    "\n",
    "PyTorch是一个基于Python的科学计算库，它有以下特点:\n",
    "\n",
    "- 类似于NumPy，但是它可以使用GPU\n",
    "- 可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用\n",
    "\n",
    "Tensors\n",
    "---------------\n",
    "\n",
    "\n",
    "Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5x3矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全部为0，类型为long的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接直接构建tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>``torch.Size`` 返回的是一个tuple</p></div>\n",
    "\n",
    "Operations\n",
    "\n",
    "\n",
    "有很多种tensor运算。我们先介绍加法运算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种着加法的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>任何in-place的运算都会以``_``结尾。\n",
    "    举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。</p></div>\n",
    "\n",
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用``torch.view``："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用``.item()``方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多阅读**\n",
    "\n",
    "\n",
    "  各种Tensor operations, 包括transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers在\n",
    "  `<https://pytorch.org/docs/torch>`.\n",
    "\n",
    "Numpy和Tensor之间的转化\n",
    "------------\n",
    "\n",
    "在Torch Tensor和NumPy array之间相互转化非常容易。\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "使用``.to``方法，Tensor可以被移动到别的device上。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "热身: 用numpy实现两层神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    # loss = (y_pred - y) ** 2\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # \n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors\n",
    "----------------\n",
    "\n",
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26488890.0\n",
      "1 24266538.0\n",
      "2 25770598.0\n",
      "3 27368496.0\n",
      "4 26048584.0\n",
      "5 20872856.0\n",
      "6 13885301.0\n",
      "7 8008642.5\n",
      "8 4328595.0\n",
      "9 2393128.5\n",
      "10 1438039.5\n",
      "11 961585.375\n",
      "12 706734.625\n",
      "13 555776.0\n",
      "14 455856.40625\n",
      "15 383453.5625\n",
      "16 327497.8125\n",
      "17 282425.1875\n",
      "18 245235.84375\n",
      "19 214042.15625\n",
      "20 187607.0625\n",
      "21 165061.265625\n",
      "22 145737.359375\n",
      "23 129097.03125\n",
      "24 114674.9375\n",
      "25 102127.1484375\n",
      "26 91183.5546875\n",
      "27 81610.9609375\n",
      "28 73201.046875\n",
      "29 65792.5078125\n",
      "30 59257.4453125\n",
      "31 53469.62109375\n",
      "32 48343.078125\n",
      "33 43784.18359375\n",
      "34 39716.7890625\n",
      "35 36085.703125\n",
      "36 32839.0078125\n",
      "37 29924.5078125\n",
      "38 27305.126953125\n",
      "39 24945.4453125\n",
      "40 22816.4140625\n",
      "41 20892.673828125\n",
      "42 19152.673828125\n",
      "43 17576.1953125\n",
      "44 16145.919921875\n",
      "45 14846.837890625\n",
      "46 13664.796875\n",
      "47 12588.1845703125\n",
      "48 11606.2724609375\n",
      "49 10710.439453125\n",
      "50 9892.0068359375\n",
      "51 9142.9921875\n",
      "52 8456.6796875\n",
      "53 7827.4765625\n",
      "54 7249.9345703125\n",
      "55 6719.5380859375\n",
      "56 6231.984375\n",
      "57 5783.31787109375\n",
      "58 5370.171875\n",
      "59 4989.3134765625\n",
      "60 4638.08447265625\n",
      "61 4313.970703125\n",
      "62 4014.80810546875\n",
      "63 3738.321044921875\n",
      "64 3482.571044921875\n",
      "65 3245.7861328125\n",
      "66 3026.46337890625\n",
      "67 2823.190673828125\n",
      "68 2634.721435546875\n",
      "69 2459.91845703125\n",
      "70 2297.60546875\n",
      "71 2146.826416015625\n",
      "72 2006.6488037109375\n",
      "73 1876.3251953125\n",
      "74 1755.068359375\n",
      "75 1642.179443359375\n",
      "76 1537.042236328125\n",
      "77 1439.096435546875\n",
      "78 1347.797119140625\n",
      "79 1262.663818359375\n",
      "80 1183.2763671875\n",
      "81 1109.1558837890625\n",
      "82 1039.95166015625\n",
      "83 975.323974609375\n",
      "84 914.9456787109375\n",
      "85 858.5068359375\n",
      "86 805.7523803710938\n",
      "87 756.412109375\n",
      "88 710.2402954101562\n",
      "89 667.0404052734375\n",
      "90 626.59423828125\n",
      "91 588.736083984375\n",
      "92 553.2673950195312\n",
      "93 520.0142822265625\n",
      "94 488.8524475097656\n",
      "95 459.64105224609375\n",
      "96 432.2497863769531\n",
      "97 406.561767578125\n",
      "98 382.4742736816406\n",
      "99 359.8756408691406\n",
      "100 338.66107177734375\n",
      "101 318.73919677734375\n",
      "102 300.0384521484375\n",
      "103 282.4784851074219\n",
      "104 265.98675537109375\n",
      "105 250.48968505859375\n",
      "106 235.92979431152344\n",
      "107 222.2437744140625\n",
      "108 209.3782501220703\n",
      "109 197.28427124023438\n",
      "110 185.9121551513672\n",
      "111 175.2185516357422\n",
      "112 165.15951538085938\n",
      "113 155.69615173339844\n",
      "114 146.79322814941406\n",
      "115 138.41232299804688\n",
      "116 130.52474975585938\n",
      "117 123.10000610351562\n",
      "118 116.10888671875\n",
      "119 109.5245361328125\n",
      "120 103.32500457763672\n",
      "121 97.48505401611328\n",
      "122 91.98748016357422\n",
      "123 86.80470275878906\n",
      "124 81.91938781738281\n",
      "125 77.31693267822266\n",
      "126 72.98050689697266\n",
      "127 68.89155578613281\n",
      "128 65.03748321533203\n",
      "129 61.40495300292969\n",
      "130 57.97856140136719\n",
      "131 54.747467041015625\n",
      "132 51.69945526123047\n",
      "133 48.82585525512695\n",
      "134 46.1154899597168\n",
      "135 43.55768966674805\n",
      "136 41.1448974609375\n",
      "137 38.86885070800781\n",
      "138 36.72059631347656\n",
      "139 34.692420959472656\n",
      "140 32.78034591674805\n",
      "141 30.974740982055664\n",
      "142 29.269718170166016\n",
      "143 27.661155700683594\n",
      "144 26.141977310180664\n",
      "145 24.707490921020508\n",
      "146 23.35318374633789\n",
      "147 22.074262619018555\n",
      "148 20.866729736328125\n",
      "149 19.726526260375977\n",
      "150 18.649076461791992\n",
      "151 17.631446838378906\n",
      "152 16.6705379486084\n",
      "153 15.762504577636719\n",
      "154 14.905073165893555\n",
      "155 14.094476699829102\n",
      "156 13.328712463378906\n",
      "157 12.604714393615723\n",
      "158 11.921358108520508\n",
      "159 11.274744987487793\n",
      "160 10.664152145385742\n",
      "161 10.08693790435791\n",
      "162 9.541687965393066\n",
      "163 9.025768280029297\n",
      "164 8.538474082946777\n",
      "165 8.07754898071289\n",
      "166 7.641746520996094\n",
      "167 7.229966163635254\n",
      "168 6.8404974937438965\n",
      "169 6.472292423248291\n",
      "170 6.124357223510742\n",
      "171 5.795223236083984\n",
      "172 5.483881950378418\n",
      "173 5.189488887786865\n",
      "174 4.910863876342773\n",
      "175 4.6474385261535645\n",
      "176 4.398386001586914\n",
      "177 4.162801742553711\n",
      "178 3.9400761127471924\n",
      "179 3.72932505607605\n",
      "180 3.5298004150390625\n",
      "181 3.3411731719970703\n",
      "182 3.162701368331909\n",
      "183 2.9936978816986084\n",
      "184 2.8340256214141846\n",
      "185 2.682847499847412\n",
      "186 2.5399045944213867\n",
      "187 2.4045395851135254\n",
      "188 2.276595115661621\n",
      "189 2.155439615249634\n",
      "190 2.0407652854919434\n",
      "191 1.9323370456695557\n",
      "192 1.8296151161193848\n",
      "193 1.7324109077453613\n",
      "194 1.6403419971466064\n",
      "195 1.5533126592636108\n",
      "196 1.4708502292633057\n",
      "197 1.3929572105407715\n",
      "198 1.3190808296203613\n",
      "199 1.2491217851638794\n",
      "200 1.1830360889434814\n",
      "201 1.1203616857528687\n",
      "202 1.061120867729187\n",
      "203 1.0049906969070435\n",
      "204 0.9518302083015442\n",
      "205 0.9015410542488098\n",
      "206 0.8538159132003784\n",
      "207 0.8088730573654175\n",
      "208 0.7661004066467285\n",
      "209 0.7256115078926086\n",
      "210 0.6872833967208862\n",
      "211 0.6511639356613159\n",
      "212 0.6168354153633118\n",
      "213 0.5843299627304077\n",
      "214 0.5534622073173523\n",
      "215 0.5243617296218872\n",
      "216 0.49673739075660706\n",
      "217 0.4706403613090515\n",
      "218 0.4458765983581543\n",
      "219 0.42239484190940857\n",
      "220 0.4002305269241333\n",
      "221 0.37914085388183594\n",
      "222 0.35920488834381104\n",
      "223 0.3404153883457184\n",
      "224 0.32252010703086853\n",
      "225 0.30557677149772644\n",
      "226 0.28955039381980896\n",
      "227 0.2743699848651886\n",
      "228 0.2599949836730957\n",
      "229 0.24635693430900574\n",
      "230 0.23341238498687744\n",
      "231 0.22120526432991028\n",
      "232 0.2095850259065628\n",
      "233 0.1985933482646942\n",
      "234 0.1882362812757492\n",
      "235 0.17837686836719513\n",
      "236 0.16906334459781647\n",
      "237 0.16022662818431854\n",
      "238 0.1518145501613617\n",
      "239 0.14390337467193604\n",
      "240 0.13636940717697144\n",
      "241 0.12925632297992706\n",
      "242 0.12249943614006042\n",
      "243 0.1161125898361206\n",
      "244 0.11005377024412155\n",
      "245 0.10430002212524414\n",
      "246 0.09886276721954346\n",
      "247 0.09368662536144257\n",
      "248 0.08881567418575287\n",
      "249 0.08418423682451248\n",
      "250 0.07980578392744064\n",
      "251 0.07562914490699768\n",
      "252 0.07169032096862793\n",
      "253 0.06795740127563477\n",
      "254 0.06440158933401108\n",
      "255 0.06107126548886299\n",
      "256 0.05788283422589302\n",
      "257 0.05486645549535751\n",
      "258 0.05202014371752739\n",
      "259 0.04932694137096405\n",
      "260 0.04675862938165665\n",
      "261 0.044336091727018356\n",
      "262 0.042029302567243576\n",
      "263 0.03984029218554497\n",
      "264 0.03776982054114342\n",
      "265 0.03582616522908211\n",
      "266 0.03395228460431099\n",
      "267 0.03219641000032425\n",
      "268 0.030540920794010162\n",
      "269 0.02894696779549122\n",
      "270 0.02746778354048729\n",
      "271 0.0260334312915802\n",
      "272 0.02470427192747593\n",
      "273 0.02340710163116455\n",
      "274 0.022209642454981804\n",
      "275 0.02105955220758915\n",
      "276 0.01998405158519745\n",
      "277 0.01894586719572544\n",
      "278 0.01797822117805481\n",
      "279 0.017050156369805336\n",
      "280 0.01618034392595291\n",
      "281 0.015336202457547188\n",
      "282 0.014552988111972809\n",
      "283 0.013814732432365417\n",
      "284 0.013101925142109394\n",
      "285 0.01243590097874403\n",
      "286 0.011799152940511703\n",
      "287 0.011202310211956501\n",
      "288 0.010631848126649857\n",
      "289 0.010097072459757328\n",
      "290 0.009581470862030983\n",
      "291 0.009097903035581112\n",
      "292 0.008642747066915035\n",
      "293 0.008199912495911121\n",
      "294 0.007785672787576914\n",
      "295 0.007397827226668596\n",
      "296 0.007030266337096691\n",
      "297 0.006672457791864872\n",
      "298 0.006342006381601095\n",
      "299 0.006028550677001476\n",
      "300 0.005728216841816902\n",
      "301 0.005448770709335804\n",
      "302 0.005185123533010483\n",
      "303 0.004930162336677313\n",
      "304 0.004690124653279781\n",
      "305 0.004462998826056719\n",
      "306 0.004242392256855965\n",
      "307 0.0040402645245194435\n",
      "308 0.0038497368805110455\n",
      "309 0.0036646986845880747\n",
      "310 0.0034928293898701668\n",
      "311 0.003331298939883709\n",
      "312 0.003171914955601096\n",
      "313 0.0030239992775022984\n",
      "314 0.002881435677409172\n",
      "315 0.002751079387962818\n",
      "316 0.00262470543384552\n",
      "317 0.0025020588655024767\n",
      "318 0.002386466832831502\n",
      "319 0.0022803398314863443\n",
      "320 0.002177285961806774\n",
      "321 0.0020770845003426075\n",
      "322 0.0019861303735524416\n",
      "323 0.0018990251701325178\n",
      "324 0.0018174622673541307\n",
      "325 0.0017388490960001945\n",
      "326 0.0016622159164398909\n",
      "327 0.0015905762556940317\n",
      "328 0.0015276595950126648\n",
      "329 0.0014594775857403874\n",
      "330 0.0013986836420372128\n",
      "331 0.001336774555966258\n",
      "332 0.0012819527182728052\n",
      "333 0.0012293698964640498\n",
      "334 0.0011793588055297732\n",
      "335 0.0011291597038507462\n",
      "336 0.0010862030321732163\n",
      "337 0.0010424524080008268\n",
      "338 0.0010012277634814382\n",
      "339 0.000961331941653043\n",
      "340 0.0009235143661499023\n",
      "341 0.0008887280127964914\n",
      "342 0.0008558207773603499\n",
      "343 0.0008234638953581452\n",
      "344 0.0007933020242489874\n",
      "345 0.0007626053411513567\n",
      "346 0.0007360494928434491\n",
      "347 0.0007093017920851707\n",
      "348 0.0006819958798587322\n",
      "349 0.0006582538480870426\n",
      "350 0.0006356279482133687\n",
      "351 0.0006134051945991814\n",
      "352 0.0005930752959102392\n",
      "353 0.000572128570638597\n",
      "354 0.0005522557185031474\n",
      "355 0.0005329162231646478\n",
      "356 0.0005160215077921748\n",
      "357 0.000498635636176914\n",
      "358 0.0004819823079742491\n",
      "359 0.00046707791625522077\n",
      "360 0.00045184194459579885\n",
      "361 0.00043631333392113447\n",
      "362 0.0004226318560540676\n",
      "363 0.00040926336077973247\n",
      "364 0.0003972171398345381\n",
      "365 0.0003837858384940773\n",
      "366 0.0003719459054991603\n",
      "367 0.0003604759695008397\n",
      "368 0.0003494161937851459\n",
      "369 0.00033922813599929214\n",
      "370 0.00032928644213825464\n",
      "371 0.0003192803415004164\n",
      "372 0.0003104244824498892\n",
      "373 0.0003021582670044154\n",
      "374 0.00029359417385421693\n",
      "375 0.00028465065406635404\n",
      "376 0.00027647888055071235\n",
      "377 0.00026940027601085603\n",
      "378 0.0002620968734845519\n",
      "379 0.0002541998110245913\n",
      "380 0.00024799961829558015\n",
      "381 0.0002411934401607141\n",
      "382 0.0002346209657844156\n",
      "383 0.00022799574071541429\n",
      "384 0.00022173169418238103\n",
      "385 0.0002170306397601962\n",
      "386 0.0002113807131536305\n",
      "387 0.00020570713968481869\n",
      "388 0.00020066114666406065\n",
      "389 0.0001953993778442964\n",
      "390 0.00019020451873075217\n",
      "391 0.0001854612783063203\n",
      "392 0.00018093138351105154\n",
      "393 0.00017655568080954254\n",
      "394 0.00017150407074950635\n",
      "395 0.00016754660464357585\n",
      "396 0.00016296334797516465\n",
      "397 0.00015966291539371014\n",
      "398 0.0001561833341838792\n",
      "399 0.00015175589942373335\n",
      "400 0.00014811332221142948\n",
      "401 0.00014426111010834575\n",
      "402 0.00014111695054452866\n",
      "403 0.00013810764357913285\n",
      "404 0.00013524365203920752\n",
      "405 0.00013244889851193875\n",
      "406 0.00012973374396096915\n",
      "407 0.00012697841157205403\n",
      "408 0.0001238826516782865\n",
      "409 0.0001213186769746244\n",
      "410 0.00011834705946967006\n",
      "411 0.00011604121391428635\n",
      "412 0.00011354991147527471\n",
      "413 0.0001111753226723522\n",
      "414 0.00010919592023128644\n",
      "415 0.00010683574510039762\n",
      "416 0.00010468952677911147\n",
      "417 0.00010295755782863125\n",
      "418 0.00010062252840725705\n",
      "419 9.860908176051453e-05\n",
      "420 9.697866335045546e-05\n",
      "421 9.496478014625609e-05\n",
      "422 9.295716881752014e-05\n",
      "423 9.116448927670717e-05\n",
      "424 8.962868741946295e-05\n",
      "425 8.774700108915567e-05\n",
      "426 8.604067261330783e-05\n",
      "427 8.442616672255099e-05\n",
      "428 8.292800339404494e-05\n",
      "429 8.13875813037157e-05\n",
      "430 8.03419534349814e-05\n",
      "431 7.848635141272098e-05\n",
      "432 7.698321132920682e-05\n",
      "433 7.557265780633315e-05\n",
      "434 7.436837768182158e-05\n",
      "435 7.298776472453028e-05\n",
      "436 7.189826283138245e-05\n",
      "437 7.04634003341198e-05\n",
      "438 6.934004341019318e-05\n",
      "439 6.843708251835778e-05\n",
      "440 6.719645898556337e-05\n",
      "441 6.600015331059694e-05\n",
      "442 6.45859690848738e-05\n",
      "443 6.364087312249467e-05\n",
      "444 6.290862802416086e-05\n",
      "445 6.157330062706023e-05\n",
      "446 6.069313531043008e-05\n",
      "447 5.9843354392796755e-05\n",
      "448 5.90069466852583e-05\n",
      "449 5.8271310990676284e-05\n",
      "450 5.7478526287013665e-05\n",
      "451 5.674186832038686e-05\n",
      "452 5.575815885094926e-05\n",
      "453 5.459640306071378e-05\n",
      "454 5.409081495599821e-05\n",
      "455 5.3372030379250646e-05\n",
      "456 5.2774186769966036e-05\n",
      "457 5.166991468286142e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458 5.102491923025809e-05\n",
      "459 5.027904626331292e-05\n",
      "460 4.939598511555232e-05\n",
      "461 4.888841795036569e-05\n",
      "462 4.8084999434649944e-05\n",
      "463 4.732015440822579e-05\n",
      "464 4.6457851567538455e-05\n",
      "465 4.582035762723535e-05\n",
      "466 4.5293101720744744e-05\n",
      "467 4.469940540730022e-05\n",
      "468 4.42201089754235e-05\n",
      "469 4.347944195615128e-05\n",
      "470 4.280045322957449e-05\n",
      "471 4.216409070068039e-05\n",
      "472 4.1591021727072075e-05\n",
      "473 4.099120997125283e-05\n",
      "474 4.04525562771596e-05\n",
      "475 4.016608727397397e-05\n",
      "476 3.953226041630842e-05\n",
      "477 3.9033231587382033e-05\n",
      "478 3.872134038829245e-05\n",
      "479 3.825015664915554e-05\n",
      "480 3.7799003621330485e-05\n",
      "481 3.711767931235954e-05\n",
      "482 3.675673360703513e-05\n",
      "483 3.635220127762295e-05\n",
      "484 3.604120502131991e-05\n",
      "485 3.57575663656462e-05\n",
      "486 3.502741310512647e-05\n",
      "487 3.452131568337791e-05\n",
      "488 3.419596760068089e-05\n",
      "489 3.3702726796036586e-05\n",
      "490 3.3460615668445826e-05\n",
      "491 3.3034772059181705e-05\n",
      "492 3.267467036494054e-05\n",
      "493 3.235387339373119e-05\n",
      "494 3.1880779715720564e-05\n",
      "495 3.152836870867759e-05\n",
      "496 3.1246145226759836e-05\n",
      "497 3.0930132197681814e-05\n",
      "498 3.050830855499953e-05\n",
      "499 3.0307808629004285e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensor和autograd\n",
    "-------------------------------\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N 是 batch size; D_in 是 input dimension;\n",
    "# H 是 hidden dimension; D_out 是 output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机的Tensor来保存输入和输出\n",
    "# 设定requires_grad=False表示在反向传播的时候我们不需要计算gradient\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 创建随机的Tensor和权重。\n",
    "# 设置requires_grad=True表示我们希望反向传播的时候计算Tensor的gradient\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播:通过Tensor预测y；这个和普通的神经网络的前向传播没有任何不同，\n",
    "    # 但是我们不需要保存网络的中间运算结果，因为我们不需要手动计算反向传播。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # 通过前向传播计算loss\n",
    "    # loss是一个形状为(1，)的Tensor\n",
    "    # loss.item()可以给我们返回一个loss的scalar\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # PyTorch给我们提供了autograd的方法做反向传播。如果一个Tensor的requires_grad=True，\n",
    "    # backward会自动计算loss相对于每个Tensor的gradient。在backward之后，\n",
    "    # w1.grad和w2.grad会包含两个loss相对于两个Tensor的gradient信息。\n",
    "    loss.backward()\n",
    "\n",
    "    # 我们可以手动做gradient descent(后面我们会介绍自动的方法)。\n",
    "    # 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，\n",
    "    # 但是在更新weights之后我们并不需要再做autograd。\n",
    "    # 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。\n",
    "    # tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，\n",
    "    # 但是不会记录计算图的历史。\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: optim\n",
    "--------------\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: 自定义 nn Modules\n",
    "--------------------------\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FizzBuzz\n",
    "\n",
    "FizzBuzz是一个简单的小游戏。游戏规则如下：从1开始往上数数，当遇到3的倍数的时候，说fizz，当遇到5的倍数，说buzz，当遇到15的倍数，就说fizzbuzz，其他情况下则正常数数。\n",
    "\n",
    "我们可以写一个简单的小程序来决定要返回正常数值还是fizz, buzz 或者 fizzbuzz。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "buzz\n",
      "fizz\n",
      "fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return 3\n",
    "    elif i % 5  == 0: return 2\n",
    "    elif i % 3  == 0: return 1\n",
    "    else:             return 0\n",
    "    \n",
    "def fizz_buzz_decode(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n",
    "\n",
    "print(fizz_buzz_decode(1, fizz_buzz_encode(1)))\n",
    "print(fizz_buzz_decode(2, fizz_buzz_encode(2)))\n",
    "print(fizz_buzz_decode(5, fizz_buzz_encode(5)))\n",
    "print(fizz_buzz_decode(12, fizz_buzz_encode(12)))\n",
    "print(fizz_buzz_decode(15, fizz_buzz_encode(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先定义模型的输入与输出(训练数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NUM_DIGITS = 10\n",
    "\n",
    "# Represent each input by an array of its binary digits.\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "trX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "trY = torch.LongTensor([fizz_buzz_encode(i) for i in range(101, 2 ** NUM_DIGITS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们用PyTorch定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "NUM_HIDDEN = 100\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了让我们的模型学会FizzBuzz这个游戏，我们需要定义一个损失函数，和一个优化算法。\n",
    "- 这个优化算法会不断优化（降低）损失函数，使得模型的在该任务上取得尽可能低的损失值。\n",
    "- 损失值低往往表示我们的模型表现好，损失值高表示我们的模型表现差。\n",
    "- 由于FizzBuzz游戏本质上是一个分类问题，我们选用Cross Entropyy Loss函数。\n",
    "- 优化函数我们选用Stochastic Gradient Descent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是模型的训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training it\n",
    "BATCH_SIZE = 128\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(trX), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = trX[start:end]\n",
    "        batchY = trY[start:end]\n",
    "\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Find loss on training data\n",
    "    loss = loss_fn(model(trX), trY).item()\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们用训练好的模型尝试在1到100这些数字上玩FizzBuzz游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', '12', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', 'fizz', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', '42', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', 'buzz', 'fizz', '70', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', 'fizz', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', 'fizzbuzz', '91', '92', 'fizz', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "# Output now\n",
    "testX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "with torch.no_grad():\n",
    "    testY = model(testX)\n",
    "predictions = zip(range(1, 101), list(testY.max(1)[1].data.tolist()))\n",
    "\n",
    "print([fizz_buzz_decode(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])))\n",
    "testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
