{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "- Deadline: 4:59pm, Monday, July 9th, 2018\n",
    "- Name: [Write down your name here]\n",
    "\n",
    "In this project, you will work on a task of sentiment classification. You will work on large movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/). The task is to classify movie reviews into two categories, POSITIVE or NEGATIVE. \n",
    "\n",
    "You are provided with a training set (TRAIN), a development set (DEV), and a test set (TEST). Your classifier is trained on TRAIN, evaluated and tuned on DEV, and tested on TEST. \n",
    "\n",
    "Your will build two classifiers in this homework, a naive bayes classifier and a logistic regression classifier with bag of words features. You have learned these two models in the lecture. We will give some additional introduction in this assignment to help you implement them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "def word_tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(53113)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file,'r') as fin:\n",
    "        for line in fin:\n",
    "            label, content = line.split(\",\", 1)\n",
    "            data.append((content.lower(), label))\n",
    "    return data\n",
    "data_dir = \"large_movie_review_dataset\"\n",
    "train_data = load_data(os.path.join(data_dir, \"train.txt\"))\n",
    "dev_data = load_data(os.path.join(data_dir, \"dev.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of TRAIN data 25000\n",
      "number of DEV data 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of TRAIN data\", len(train_data))\n",
    "print(\"number of DEV data\", len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a generic model class as below. The model has 2 functions, train and classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the training data\n",
    "        self.vocab = Counter([word for content, label in data for word in word_tokenize(content)]).most_common(VOCAB_SIZE-1) \n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in enumerate(self.vocab)} # word to index mapping\n",
    "        self.word_to_idx[\"UNK\"] = 0 # all the unknown words will be mapped to index 0\n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        classify the documents with the model\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Bag of Words\n",
    "\n",
    "You will implement logistic regression with bag of words features in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provide a common dataset interface. \n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.zeros(self.vocab_size)\n",
    "        \n",
    "        item = torch.from_numpy(item)\n",
    "        if len(self.data[idx]) == 2: # in training or evaluation, we have both the document and label\n",
    "            for word in word_tokenize(self.data[idx][0]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            label = self.label_to_idx[self.data[idx][1]]\n",
    "            return item, label\n",
    "        else: # in testing, we only have the document without label\n",
    "            for word in word_tokenize(self.data[idx]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "class BoWLRClassifier(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, data)\n",
    "        \n",
    "        '''\n",
    "        In this model initialization phase, you will do the following: \n",
    "        1. Define a linear layer to transform bag of words features into 2 classes. \n",
    "        2. Define the loss function, you will use cross entropy loss\n",
    "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss\n",
    "        3. Define an optimizer for the model, you may choose to use SGD, Adam or other optimizers you know\n",
    "            https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD\n",
    "        '''\n",
    "        # TODO\n",
    "        # pass\n",
    "        self.linear = nn.Linear(VOCAB_SIZE, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "    def forward(self, bow):\n",
    "        '''\n",
    "        Run the model. You may only need to run the linear layer defined in the init function. \n",
    "        '''\n",
    "        return self.linear(bow)\n",
    "    \n",
    "    def train_epoch(self, train_data):\n",
    "        '''\n",
    "        Train the model for one epoch with the training data\n",
    "        When training a model, you will repeat the following procedures:\n",
    "        1. get one batch of features and labels\n",
    "        2. make a forward pass with the features to get predictions\n",
    "        3. calculate the loss with the predictions and target labels\n",
    "        4. run a backward pass from the loss function to get the gradients\n",
    "        5. apply the optimizer step to update the model paramters\n",
    "        '''\n",
    "        dataset = TextClassificationDataset(self.word_to_idx, train_data)\n",
    "        dataloader = tud.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "        self.train()\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            preds = self.forward(X)\n",
    "            loss = self.loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            if i % 500 == 0:\n",
    "                print(\"loss: {}\".format(loss.item()))\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def train_model(self, train_data, dev_data):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, you will evaluate your model on the DEV set. \n",
    "        The best performing model on the DEV set shall be saved to best_model\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(2):\n",
    "            self.train_epoch(train_data)\n",
    "            dev_acc = self.evaluate(dev_data)\n",
    "            print(\"dev acc: {}\".format(dev_acc))\n",
    "            if dev_acc > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc)\n",
    "\n",
    "    def classify(self, docs):\n",
    "        '''\n",
    "        This function classifies documents into their categories. \n",
    "        docs are documents only, without labels.\n",
    "        '''\n",
    "        dataset = TextClassificationDataset(self.word_to_idx, docs)\n",
    "        dataloader = tud.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, X in enumerate(dataloader):\n",
    "                X = X.float()\n",
    "                if torch.cuda.is_available():\n",
    "                    X = X.cuda()\n",
    "                preds = self.forward(X)\n",
    "                results.append(preds.max(1)[1].cpu().numpy().reshape(-1))\n",
    "        results = np.concatenate(results)\n",
    "        results = [self.idx_to_label[p] for p in results]\n",
    "        return results\n",
    "                \n",
    "    def evaluate(self, data):\n",
    "        '''\n",
    "        This function evaluate the data with the current model. \n",
    "        data contains documents and labels. \n",
    "        It calls function \"classify\" to make predictions, \n",
    "        and compare with the correct labels to return the model accuracy on \"data\". \n",
    "        '''\n",
    "        self.eval()\n",
    "        preds = self.classify([d[0] for d in data])\n",
    "        targets = [d[1] for d in data]\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        for p, t in zip(preds, targets):\n",
    "            if p == t: \n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct/total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7611655592918396\n",
      "loss: 0.45513585209846497\n",
      "loss: 0.4699181616306305\n",
      "loss: 0.6984012126922607\n",
      "loss: 0.169261172413826\n",
      "loss: 0.38961079716682434\n",
      "loss: 0.3471197187900543\n",
      "dev acc: 0.859\n",
      "loss: 0.25644347071647644\n",
      "loss: 0.5411393046379089\n",
      "loss: 0.4370480477809906\n",
      "loss: 0.23390710353851318\n",
      "loss: 0.3318834900856018\n",
      "loss: 0.22103242576122284\n",
      "loss: 0.6189835071563721\n",
      "dev acc: 0.8514\n"
     ]
    }
   ],
   "source": [
    "lr_model = BoWLRClassifier(train_data)\n",
    "if torch.cuda.is_available():\n",
    "    lr_model = lr_model.cuda()\n",
    "lr_model.train_model(train_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now spend some time to tune your models. At least try the following: \n",
    "\n",
    "- try another optimizer\n",
    "- change the learning rate\n",
    "- change the number of epochs to train\n",
    "\n",
    "Report your results and analysis in the writeup. \n",
    "\n",
    "Finally, make predictions on the TEST set, and submit your predictions to out [Kaggle competition page](https://www.kaggle.com/c/mpcs-53113-hw1-logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = lr_model.classify(test_data)\n",
    "write_to_file(preds, \"lr_test_preds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 10 features with the maximum weights for POSITIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excellent',\n",
       " 'great',\n",
       " 'wonderful',\n",
       " 'favorite',\n",
       " 'amazing',\n",
       " 'perfect',\n",
       " 'definitely',\n",
       " 'best',\n",
       " 'loved',\n",
       " 'highly']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lr_model.linear.weight.data.cpu().numpy()[0]\n",
    "pos_indices = weights.argsort()[-10:][::-1]\n",
    "[lr_model.idx_to_word[i] for i in pos_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 10 features with the maximum negative weights for POSITIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'waste',\n",
       " 'awful',\n",
       " 'bad',\n",
       " 'boring',\n",
       " 'poorly',\n",
       " 'poor',\n",
       " 'nothing',\n",
       " 'bad.',\n",
       " 'worse']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_indices = weights.argsort()[:10]\n",
    "[lr_model.idx_to_word[i] for i in pos_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worse',\n",
       " 'bad.',\n",
       " 'nothing',\n",
       " 'poor',\n",
       " 'poorly',\n",
       " 'boring',\n",
       " 'bad',\n",
       " 'awful',\n",
       " 'waste',\n",
       " 'worst']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_indices = lr_model.linear.weight.data.cpu().numpy()[0].argsort()[:10][::-1]\n",
    "idx_to_word = {v:k for k, v in lr_model.word_to_idx.items()}\n",
    "[idx_to_word[i] for i in pos_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 10 features with the maximum positive weights for NEGATIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waste',\n",
       " 'worst',\n",
       " 'awful.',\n",
       " 'poorly',\n",
       " 'terrible.',\n",
       " 'forgettable',\n",
       " 'fails',\n",
       " 'horrible.',\n",
       " 'awful',\n",
       " 'disappointing']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lr_model.linear.weight.data.cpu().numpy()[1]\n",
    "pos_indices = weights.argsort()[-10:][::-1]\n",
    "[lr_model.idx_to_word[i] for i in pos_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 10 features with the maximum negative weights for NEGATIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['superbly',\n",
       " 'excellent',\n",
       " 'favorite',\n",
       " 'excellent.',\n",
       " 'perfect.',\n",
       " 'refreshing',\n",
       " 'perfect,',\n",
       " 'amazing.',\n",
       " '8/10',\n",
       " 'wonderfully']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_indices = lr_model.linear.weight.data.cpu().numpy()[1].argsort()[:10][::-1]\n",
    "idx_to_word = {v:k for k, v in lr_model.word_to_idx.items()}\n",
    "[idx_to_word[i] for i in pos_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
